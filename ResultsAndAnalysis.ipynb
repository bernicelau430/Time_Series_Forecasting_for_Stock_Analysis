{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOdugex3xNk-"
   },
   "source": [
    "# Results and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlNphCh46-yG"
   },
   "source": [
    "## Analysis on Model Performance\n",
    "Which model works best in time series forecasting for stock predictions?\n",
    "\n",
    "That is, which model produces the most accurate stock price predictions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9XRDHx07MkL"
   },
   "source": [
    "### Summary Statistics\n",
    "\n",
    "Comparison of RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaYMF1m07d_p"
   },
   "source": [
    "|                         | LSTM  | KNN   | Random Forest |\n",
    "|-------------------------|-------|-------|---------------|\n",
    "| Root Mean Squared Error | 7.14  | 11.13 | 12.24         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rj-5nB7F8yO5"
   },
   "source": [
    "Based on the RMSE of all three models, LSTM (Long Short Term Memory) works best in time series forecasting for stock predictions. Looking at the Root Mean Squared Error for each regression model, we can see that LSTM has the lowest value. Let's first explore why this is..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction Visualization\n",
    "\n",
    "We can first look at a visualization of the predictions our models made with the test data compared to the actual stock prices to get a better understanding of how models performed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long Short Term Memory\n",
    "\n",
    "<img src=\"images/lstm_pred.png\" alt=\"Long Short Term Memory\" width=\"800\"/>\n",
    "\n",
    "It is obvious here that this model did very well. While the test and predictions are not exactly the same, we can see that the model was able to predict the general upwards and downwards trends of the stock close price. Considering how LSTM works, this makes sense. LSTM has the ability to capture trends and fluctuations in time-series data, making is very efficient for predictions with complex time dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest-Neighbors\n",
    "\n",
    "<img src=\"images/knn_pred.png\" alt=\"K Nearest Neighbors\" width=\"800\"/>\n",
    "\n",
    "This model did not do very well. As visualized, the KNN predicted a horizontal line for the predicted values which does not match well with the actual stock close prices. The issue with KNN is that it is weak for high-dimensional data and complex patterns. When trying to predict time-series forecasting, KNN is unable to consider / analyze long-term trends which is needed to predict stock patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "<img src=\"images/rf_pred.png\" alt=\"Random Forest\" width=\"800\"/>\n",
    "\n",
    "Random forest looks better than KNN as it is not just a horizontal line. However, we can see here that it does not accurately  predict the correct trends (predicted price drop when price actually increased, predicted no change when price actually increased). Random forest has some issues because it lacks long-term memory and ability to see sequences. Each day is treated as independent, so it is very difficult to capture trends between days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Prediction Accuracy Visualization\n",
    "\n",
    "To support the visualizations shown above, we can look at the plot of residuals between our test and predicted stock close prices. Residuals = actual values - predicted values. Thus, we are looking for random distribution near or on the line y = 0. If values are very far off from y = 0, then the model did not do a good job predicting. If the values are not randomly distributed, the model is inadequate because that tells us the model is outputing average as its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Long Short Term Memory\n",
    "\n",
    "<img src=\"images/lstm_res.png\" alt=\"Long Short Term Memory\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Nearest-Neighbors\n",
    "\n",
    "As KNN predicts stock close prices, it flattens out and predicts the same price every time. Thus, a distribution of residuals does not tell us anything significant. This, we decided to leave out this graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "<img src=\"images/rf_res.png\" alt=\"Random Forest\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, the best classification model to predict stock close prices for NVIDIA stock is Long Short Term Memory. As seen above, Long Short Term Memory had a significantly smaller mean squared error, even distribution of residuals, and low residual values. This is due to a couple reasons...\n",
    "\n",
    "Long Term Short Memory\n",
    "- LSTM is a recurrent neural network is designed to handle sequential data. Thus, it is ideal for time-series forecasting and predicting stock prices. Since this model uses memory cells, it can remember relevant data, even over long periods of time.\n",
    "- Since stock prices are highly sequential, LSTM can easily capture trends such as patterns and fluctuations. Thus, it would make sense that this model performs the best for this specific task.\n",
    "\n",
    "K-Nearest-Neighbors\n",
    "- KNN is a very simple algorithm that predicts the value of new data points based on the points that are closest to it. It basically calculates the Euclidean distance between target points and looks for the most similar instances.\n",
    "- This model has significant limitations...\n",
    "    1. Static, non generational approach\n",
    "        - KNN does not consider the sequence or ordering of previous points. Due to how the algorithm functions, KNN has no way to capture trends or dependences of previous data points. In trying to predict on time series data, capturing trends is the most important thing.\n",
    "    2. Lagging predictions\n",
    "        - Since KNN essentially pattern matches to make predictions, it is not a great option for predicting stock prices. If the price changes drastically in a short period of time, KNN will often fall behind on its prediction because it cannot predict something out of the ordinary. \n",
    "- These limitations were explicitly depicted in the result of our KNN model. This model predicted a horizonal line for the stock prices. This tells us that KNN was basically just taking the average of the closest neighbors and not capturing any real trends of patterns. It is obvious why this is not ideal for predicting stock close prices, and tells us that this was not the correct machine learning model to use for time series forecasting on Nvidia stock close prices.\n",
    "\n",
    "Random Forest\n",
    "- RF is a model based on many different decision trees. Each tree is trained on a subset of data which can allow it to capture patterns of the data. However, the final prediction is typically the average of predictions from each tree\n",
    "- This model too has significant limitations...\n",
    "    1. Feature Dependency\n",
    "        - Random Forest is essentially a combination of many different decision trees that allow the machine to predict based on many features what the target will be. Thus, Random Forest depends on many features to make an accurate prediction. That is not what we have here. We are predicting on previous stock prices and trying to capture trends, not multiple features that can influence stock prices. \n",
    "    2. Averaging Effect\n",
    "        - With the many decision trees that are combined to create the random forest, the model must use all the values that come from each tree. Thus, it takes the average of the values. Taking the average often causes the prediction to \"smooth out,\" something that can be seen in the visualization of its prediction. This is a problem because stocks jump up and down, so a horizonal prediction will almost never be correct.\n",
    "- Thus, Random Forest is not a good option for time series forecasting. Since it depends on many different features and uses tree averages to make a prediction, stock price trends are not captured and thus the predicted stock values are not adequate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Exploration: LSTM Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding further on our best-performing model, we experimented with different amounts and types of layers, such as LSTM and Dropout layers, as well as varying units per layer, to see if certain parameter combinations would perform better than our initial model. We found that the Root Mean Squared Error changed as follows..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                         | Added LSTM layers | Increased Units per layer | Added Dropout layers | Chaning optimizer to RMSprop | Increasing batch size / disabling shuffle | Changing prediction horizon to 50\n",
    "|-------------------------|-------------------|---------------------------|----------------------|----------------------|----------------------|----------------------|\n",
    "| Root Mean Squared Error | 6.42              | 7.56                      | 9.20                 |3.87                 |9.62                 |11.95                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Added Long Short Term Memory layers\n",
    "- LSTM layers are a type of recurrent neural network. It is most useful for sequential data with important previous trends. \n",
    "- By adding more LSTM layers, we are able to detect more complex patterns which is of upmost important when predicting stock close prices. Thus, we were able to reduce our RMSE (Root Mean Square Error) from 7.41 to 6.42, a significant change that suggests deeper and more complex models can be more accurate.\n",
    "\n",
    "#### Increased Units Per Layer\n",
    "- Adding more units per LSTM layer gives the model the ability to learn more from each unit of data, as significant trends can be small in change\n",
    "- However, this resulted in a slightly higher RMSE value meaning the model was less accurate. This tells us that the model most likely overfit the data causing it to focus too strongly on insignificant trends. \n",
    "\n",
    "#### Adding Dropout Layers\n",
    "- Adding dropout layers is a technique that we can use to help prevent overfitting. The general idea is that if we prevent the model from relying on small trend, it can capture the general trend better. \n",
    "- Unfortunately, this caused the model to perform the worse, raising the RMSE to 9.2. This tells us that adding these layers caused the model to generalize too much and miss out on important underlying trends. \n",
    "\n",
    "#### Changing the optimizer (RMSprop)\n",
    "- Changing the optimizer to Root Mean Square Propagation allows the model to adapt the learning rate for every parameter. This is especially useful for training neural networks on time-series data. Tt divides the learning rate by the average of gradients to help improve convergence of the data.\n",
    "- This allowed the model to result on a RMSE of 3.38, our best yet. This suggests that using RMSprop allows LSTM to better handle the variance and noise of NVIDIA stock prices.\n",
    "\n",
    "#### Increasing batch size and disabled shuffle\n",
    "- Increasing the batch size allows the model to process more data simultaneously during every training step to help improve stability of the model. Disabling shuffle ensures that the data is trained in sequential order, something that is very important for time series data.\n",
    "- Unfortunately, this resulted in a higher RMSE of 9.62 meaning our model was less accurate. This is most likely due to the fact that processing large data chunks causes the model to miss important but less obvious trends.\n",
    "\n",
    "#### Changing prediction horizon to 50\n",
    "\n",
    "- Changing the prediction horizon allows the model to look further into the future to make its prediction. By setting the horizon to 50, the model is attempting to forecast further in the future. This can help the model focus on broader trends rather than short-term fluctuations.\n",
    "- This caused another increased in RMSE giving us 11.95. Notably, this model simply predicted a flat line. This suggests that by increasing complexity to our model, it was not able to maintain accuracy.\n",
    "\n",
    "----------------------\n",
    "\n",
    "Thus, our best option is to change the optimizer to RMSprop. By adapting the learning rate for each parameter, we can capture the most important trends without overfitting the data. We can see in the following visualization of predicted vs. actual values that the model did a good job at predicting the Nvidia stock close prices.\n",
    "\n",
    "<img src=\"images/lstm_final.png\" alt=\"Long Short Term Memory\" width=\"600\"/>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
